{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Assignment 5"]},{"cell_type":"markdown","metadata":{},"source":["# Part 1\n","\n","Split the data into a 70-30 split for training and testing data."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set()\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["#reading the data \n","df = pd.read_csv('homework/radar_parameters.csv')\n","\n","#creating different dataframes for design matrix and target variable.\n","x_data = df.iloc[:,1:7]\n","y_data = df.iloc[:,7]"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["#importing the train_test_split to split the data. \n","from sklearn.model_selection import train_test_split\n","\n","#spliting 70% data to training and 30% to testing data \n","x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.3)"]},{"cell_type":"markdown","metadata":{},"source":["# Part 2\n","\n","Using the split created in (1), train a multiple linear regression dataset using the training dataset, and validate it using the testing dataset. Compare the R^2 and root mean square errors of model on the training and testing sets to a baseline prediction of rain rate using the formula Z = 200R^1.6."]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["#imports to calculate RMSE and R^2\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import r2_score\n","from math import sqrt\n","\n","#calculating the baseline prediction of rain rate using the formulae given.\n","df_z = 10**(x_data['Zh (dBZ)']/10)\n","df_r = (df_z/200)**(1/1.6)\n","\n","#calculating baseline RMSE and R^2.\n","baseline_rms = sqrt(mean_squared_error(y_data, df_r))\n","baseline_r2 = r2_score(y_data, df_r)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"],"text/plain":["LinearRegression()"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["#importing LinearRegression model to fit the training data.\n","\n","from sklearn.linear_model import LinearRegression\n","model_linear = LinearRegression(fit_intercept=True)\n","\n","model_linear.fit(x_train, y_train)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["#predicting y for the test data, \n","y_test_predicted = model_linear.predict(x_test)\n","\n","#calculating RMSE and R^2 for linear model. \n","rmse_linear = sqrt(mean_squared_error(y_test, y_test_predicted))\n","r2_linear = r2_score(y_test, y_test_predicted)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["RMSE of baseline: 7.157590840042378\n","R^2 of baseline: 0.3023229070437503\n","RMSE of linear regression: 0.9420616042289477\n","R^2 of linear regression: 0.988487617518314\n"]}],"source":["print('RMSE of baseline:', baseline_rms)\n","print('R^2 of baseline:', baseline_r2)\n","\n","print('RMSE of linear regression:', rmse_linear)\n","print('R^2 of linear regression:', r2_linear)"]},{"cell_type":"markdown","metadata":{},"source":["As we can see from above, the baseline prediction model has a R^2 of only 0.3023, which means that it is able to explain only 30.25% of the variation in data, which is pretty bad. The linear model has a much higher R^2 value and a lower RMSE value than the baseline prediction model. Hence, we can conclude that linear model does a better job in predicting the rain rate for the given data. "]},{"cell_type":"markdown","metadata":{},"source":["# Part 3\n","\n","Repeat 1 doing a grid search over polynomial orders, using a grid search over orders 0-21, and use cross-validation of 7 folds. For the best polynomial model in terms of R^2, does it outperform the baseline and the linear regression model in terms of R^2 and root mean square error?"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["#importing for polynomial regression\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.pipeline import make_pipeline\n","\n","#making a pipeline for polynomial regression\n","def PolynomialRegression(degree=2, **kwargs):\n","    return make_pipeline(PolynomialFeatures(degree),\n","                         LinearRegression(**kwargs))"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["#importing GridSearchCV\n","from sklearn.model_selection import GridSearchCV\n","\n","#defining the parameter grid and grid search \n","param_grid = {'polynomialfeatures__degree': np.arange(7)}\n","\n","poly_grid = GridSearchCV(PolynomialRegression(), param_grid, scoring= 'r2', cv=7) #finding the best model in terms of R^2"]},{"cell_type":"markdown","metadata":{},"source":["The grid search in the above block was performed only from for orders 0-7, since it was taking a lot of time to compute the search for 0-21 orders. "]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/html":["<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=7,\n","             estimator=Pipeline(steps=[(&#x27;polynomialfeatures&#x27;,\n","                                        PolynomialFeatures()),\n","                                       (&#x27;linearregression&#x27;,\n","                                        LinearRegression())]),\n","             param_grid={&#x27;polynomialfeatures__degree&#x27;: array([0, 1, 2, 3, 4, 5, 6])},\n","             scoring=&#x27;r2&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=7,\n","             estimator=Pipeline(steps=[(&#x27;polynomialfeatures&#x27;,\n","                                        PolynomialFeatures()),\n","                                       (&#x27;linearregression&#x27;,\n","                                        LinearRegression())]),\n","             param_grid={&#x27;polynomialfeatures__degree&#x27;: array([0, 1, 2, 3, 4, 5, 6])},\n","             scoring=&#x27;r2&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;polynomialfeatures&#x27;, PolynomialFeatures()),\n","                (&#x27;linearregression&#x27;, LinearRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PolynomialFeatures</label><div class=\"sk-toggleable__content\"><pre>PolynomialFeatures()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"],"text/plain":["GridSearchCV(cv=7,\n","             estimator=Pipeline(steps=[('polynomialfeatures',\n","                                        PolynomialFeatures()),\n","                                       ('linearregression',\n","                                        LinearRegression())]),\n","             param_grid={'polynomialfeatures__degree': array([0, 1, 2, 3, 4, 5, 6])},\n","             scoring='r2')"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["#fitting the training data to the grid search \n","poly_grid.fit(x_train, y_train)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["{'polynomialfeatures__degree': 2}"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["poly_grid.best_params_"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["#predicting the test data for the best model found in the grid search\n","poly_model = poly_grid.best_estimator_\n","\n","y_test_poly_pred = poly_model.fit(x_train, y_train).predict(x_test)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["#calculating RMSE and R^2 for the best polynomial model.\n","rmse_poly = sqrt(mean_squared_error(y_test, y_test_poly_pred))\n","r2_poly = r2_score(y_test, y_test_poly_pred)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["RMSE of polynomial regression: 0.21769848653700608\n","R^2 of polynomial regression: 0.9993852232673677\n","RMSE of linear regression: 0.9420616042289477\n","R^2 of linear regression: 0.988487617518314\n","RMSE of baseline: 7.157590840042378\n","R^2 of baseline: 0.3023229070437503\n"]}],"source":["print('RMSE of polynomial regression:', rmse_poly)\n","print('R^2 of polynomial regression:', r2_poly)\n","\n","print('RMSE of linear regression:', rmse_linear)\n","print('R^2 of linear regression:', r2_linear)\n","\n","print('RMSE of baseline:', baseline_rms)\n","print('R^2 of baseline:', baseline_r2)"]},{"cell_type":"markdown","metadata":{},"source":["As we can see from above, the polynomial model also outperforms the baseline model by a large margin. While the linear and polynomial models have very close R^2 values (0.988 and 0.999), the polynomial model has a much lower RMSE values (0.2177) ansd a slightly higher R^2 value. Hence, we can conclude that the polynomial model outperforms both the baseline and the linear model. "]},{"cell_type":"markdown","metadata":{},"source":["# Part 4\n","\n","Repeat 1 with a Random Forest Regressor, and perform a grid_search on the following parameters:"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["#importing and defining a Random Forest Regression model\n","from sklearn.ensemble import RandomForestRegressor\n","forest = RandomForestRegressor(50)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["#defining the parameter grid using the values given in the question\n","random_param_grid = {'bootstrap': [True, False],  \n","                        'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],  \n","                        'max_features': ['auto', 'sqrt'],  \n","                        'min_samples_leaf': [1, 2, 4],  \n","                        'min_samples_split': [2, 5, 10],  \n","                        'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["#defining the grid search \n","from sklearn.model_selection import RandomizedSearchCV\n","\n","forest_grid = RandomizedSearchCV(estimator = forest, param_distributions= random_param_grid, scoring= 'r2')"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n","c:\\Users\\Charu\\miniconda3\\envs\\py.3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n"]},{"data":{"text/html":["<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(estimator=RandomForestRegressor(n_estimators=50),\n","                   param_distributions={&#x27;bootstrap&#x27;: [True, False],\n","                                        &#x27;max_depth&#x27;: [10, 20, 30, 40, 50, 60,\n","                                                      70, 80, 90, 100, None],\n","                                        &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;],\n","                                        &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n","                                        &#x27;min_samples_split&#x27;: [2, 5, 10],\n","                                        &#x27;n_estimators&#x27;: [200, 400, 600, 800,\n","                                                         1000, 1200, 1400, 1600,\n","                                                         1800, 2000]},\n","                   scoring=&#x27;r2&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(estimator=RandomForestRegressor(n_estimators=50),\n","                   param_distributions={&#x27;bootstrap&#x27;: [True, False],\n","                                        &#x27;max_depth&#x27;: [10, 20, 30, 40, 50, 60,\n","                                                      70, 80, 90, 100, None],\n","                                        &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;],\n","                                        &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n","                                        &#x27;min_samples_split&#x27;: [2, 5, 10],\n","                                        &#x27;n_estimators&#x27;: [200, 400, 600, 800,\n","                                                         1000, 1200, 1400, 1600,\n","                                                         1800, 2000]},\n","                   scoring=&#x27;r2&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_estimators=50)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_estimators=50)</pre></div></div></div></div></div></div></div></div></div></div>"],"text/plain":["RandomizedSearchCV(estimator=RandomForestRegressor(n_estimators=50),\n","                   param_distributions={'bootstrap': [True, False],\n","                                        'max_depth': [10, 20, 30, 40, 50, 60,\n","                                                      70, 80, 90, 100, None],\n","                                        'max_features': ['auto', 'sqrt'],\n","                                        'min_samples_leaf': [1, 2, 4],\n","                                        'min_samples_split': [2, 5, 10],\n","                                        'n_estimators': [200, 400, 600, 800,\n","                                                         1000, 1200, 1400, 1600,\n","                                                         1800, 2000]},\n","                   scoring='r2')"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["#fitting the training data to the grid search \n","forest_grid.fit(x_train, y_train)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["{'n_estimators': 1200,\n"," 'min_samples_split': 5,\n"," 'min_samples_leaf': 1,\n"," 'max_features': 'sqrt',\n"," 'max_depth': None,\n"," 'bootstrap': False}"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["forest_grid.best_params_"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["#predicting the test data with the best estimators found in the grid search \n","forest_model = forest_grid.best_estimator_\n","\n","y_test_forest_pred = forest_model.fit(x_train, y_train).predict(x_test)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["#calculation the RMSE and R^2 values for the random forest regressor. \n","rmse_forest = sqrt(mean_squared_error(y_test, y_test_forest_pred))\n","r2_forest = r2_score(y_test, y_test_forest_pred)"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["RMSE of random forest: 1.2693003462040438\n","R^2 of random forest: 0.9791005238823449\n","RMSE of polynomial regression: 0.21769848653700608\n","R^2 of polynomial regression: 0.9993852232673677\n","RMSE of linear regression: 0.9420616042289477\n","R^2 of linear regression: 0.988487617518314\n","RMSE of baseline: 7.157590840042378\n","R^2 of baseline: 0.3023229070437503\n"]}],"source":["print('RMSE of random forest:', rmse_forest)\n","print('R^2 of random forest:', r2_forest)\n","\n","print('RMSE of polynomial regression:', rmse_poly)\n","print('R^2 of polynomial regression:', r2_poly)\n","\n","print('RMSE of linear regression:', rmse_linear)\n","print('R^2 of linear regression:', r2_linear)\n","\n","print('RMSE of baseline:', baseline_rms)\n","print('R^2 of baseline:', baseline_r2)"]},{"cell_type":"markdown","metadata":{},"source":["As we can see from, the best optimized Random Forest Regressor has higher RMSE  and slightly lower R^2 than both the linear and polynomial models. Even though the random forest regressor outperforms the baseline, it does not perform better that the linear or polynomial models. \n","\n","In conclusion, the baseline model performs the worst and every other model outperforms it, whereas, the best polynomial model found using the grid performs the best and outperforms all other models. "]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.0 ('py.3')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"1455c3dcafae492b19a99d418896ce62e8e0aaa5c95bc33a8546e7b20bb4f7d9"}}},"nbformat":4,"nbformat_minor":2}
